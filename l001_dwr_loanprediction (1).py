# -*- coding: utf-8 -*-
"""L001_DWR_LoanPrediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZTg6WfdMF-uRVVXsISDKQ_WUKDoA-qeb
"""

import pandas as pd
import seaborn as sns
import numpy as np
filepath="/content/train.csv"
df=pd.read_csv(filepath)

display(df)
df.shape

df.info()

df.describe()

df.isnull().sum()

df.shape[0]

df.isnull().sum()/df.shape[0]*100

sns.distplot(df['ApplicantIncome'])

sns.boxplot(df['ApplicantIncome'])

df.Education.value_counts(normalize=True)*100

df["total_income"]=df["ApplicantIncome"]+df["CoapplicantIncome"]
df.total_income

df.describe()

sns.distplot(df['total_income'])

sns.boxplot(df['total_income'])

sns.distplot(np.log(df['total_income']))

sns.distplot(np.sqrt(df['total_income']))

df["ti_log"]=np.log(df["total_income"])

df.describe()

sns.boxplot(df['ti_log'])

m=np.mean(df["ti_log"])
sd=np.std(df["ti_log"])
p99=np.percentile(df['ti_log'],99)
IQR=np.percentile(df['ti_log'],75)-np.percentile(df['ti_log'],25)
print(m,sd,p99,IQR)

print(f"mean+3*sd : {df.loc[df['ti_log'] > m+3*sd,'Loan_ID'].count()}")

print(f"1.5*IQR: {df.loc[df['ti_log'] > 3*IQR, 'Loan_ID'].count()}")

print(f"p99: {df.loc[df['ti_log'] > p99, 'Loan_ID'].count()}")

df["ti_log"]=np.where(df["ti_log"]>p99,p99,df["ti_log"])

df["ti_log"].describe()

sns.distplot(df['ti_log'])

m=np.mean(df.LoanAmount.dropna())
sd=np.std(df.LoanAmount.dropna())
p99=np.percentile(df.LoanAmount.dropna(),99)
iqr=np.percentile(df.LoanAmount.dropna(),75)-np.percentile(df.LoanAmount.dropna(),25)
print(m,sd,p99,iqr)

df["LoanAmount"]=np.where(df["LoanAmount"]>p99,p99,df["LoanAmount"])
sns.distplot(df['LoanAmount'])

sns.distplot(df['Loan_Amount_Term'])

df.Loan_Amount_Term.value_counts(normalize=True)*100

df.isnull().sum()

df.describe()

df.Credit_History.fillna('MISSING').value_counts(normalize=True)*100

df.Credit_History.fillna(0,inplace=True)

df.Credit_History.fillna('MISSING').value_counts(normalize=True)*100

df.Gender.value_counts(normalize=True)*100

pd.crosstab(df.Gender.fillna('MISSING'),df.Education)

sns.violinplot(x=df.Gender.fillna('MISSING'),y=df.ApplicantIncome)

pd.crosstab(df.Gender.fillna('MISSING'),df.Married)

df.loc[(df.Married=="Yes") & (df.Gender.isnull()),"Gender"]="Male"

df.Gender.isnull().sum()

df.Gender.fillna("Male",inplace=True)

df.Gender.isnull().sum()

df.Married.value_counts(normalize=True)*100

pd.crosstab(df.Married.fillna('MISSING'),df.Gender)

df.loc[(df.Gender=="Male") & (df.Married.isnull()),"Married"]="Yes"

df.loc[(df.Gender=="Female") & (df.Married.isnull()),"Married"]="No"

pd.crosstab(df.Married.fillna('MISSING'),df.Gender)

sns.violinplot(x=df.Dependents.fillna('MISSING'),y=df.total_income)

df.Dependents.fillna(2,inplace=True)

sns.violinplot(x=df.Dependents.fillna('MISSING'),y=df.total_income)

pd.crosstab(df.Self_Employed.fillna('MISSING'),df.Education)

sns.violinplot(x=df.Self_Employed.fillna('MISSING'),y=df.LoanAmount)

pd.crosstab(df.Self_Employed.fillna('MISSING'),df.Credit_History)

sns.scatterplot(x=df.LoanAmount,y=df.ApplicantIncome)

df.groupby(['Gender'])["LoanAmount"].mean()

df.groupby(['Education'])["LoanAmount"].mean()

df.groupby(['Self_Employed'])["LoanAmount"].mean()

df.groupby(['Married'])["LoanAmount"].mean()

df['LoanAmount'].fillna(df.groupby(['Gender'])['LoanAmount'].transform('mean'),inplace=True)

df.Self_Employed.fillna('Yes',inplace=True)

df.Loan_Amount_Term.value_counts(normalize=True)*100

df.Loan_Amount_Term.fillna(df.Loan_Amount_Term.mode()[0], inplace=True)

df.isnull().sum()

df1=df.copy()

df2=pd.read_csv('/content/test_lAUu6dG.csv')

df2["total_income"]=df2["ApplicantIncome"]+df2["CoapplicantIncome"]
df2.total_income
df2.head()

df2['TI_log']= np.log((df2.total_income))
df2.head()

p99_2=np.percentile((df2.TI_log),99)
p99_2

df2['TI_log']= np.where((df2.TI_log)> p99_2, p99_2, (df2.TI_log))
df2.describe()

p99_3=np.percentile((df2.LoanAmount.dropna()),99)
p99_3

df2['LoanAmount']= np.where(df2['LoanAmount']> p99_3, p99_3, df2['LoanAmount'])
df2.describe()

df2.Credit_History.fillna('MISSING').value_counts(normalize=True)*100

df2.Credit_History.fillna(0,inplace=True)

df2.Credit_History.value_counts(normalize=True)*100

df2.loc[(df2.Married=="Yes")&(df2.Gender.isnull()),'Gender']="Male"

df2.Gender.fillna("Male",inplace=True)

df2.loc[(df2.Gender=="Male")&(df2.Married.isnull()),'Married']="Yes"

df2.loc[(df2.Gender=="Female")&(df2.Married.isnull()),'Married']="No"

df2.Dependents.fillna("2",inplace=True)

df2.groupby(['Gender'])['LoanAmount'].mean()

df2.groupby(['Education'])['LoanAmount'].mean()

df2.groupby(['Credit_History'])['LoanAmount'].mean()

df2.groupby(['Married'])['LoanAmount'].mean()

df2['LoanAmount'].fillna(df2.groupby('Education')['LoanAmount'].transform('mean'),inplace=True)

df2.Self_Employed.fillna('Yes',inplace=True)

df2.Loan_Amount_Term.fillna(df2.Loan_Amount_Term.mode()[0],inplace=True)

df2_2=df2.drop('Loan_ID',axis=1)

df1_1=df1.drop('Loan_ID',axis=1)

X_train=df1_1.drop('Loan_Status',axis=1)

Y_train=df1_1["Loan_Status"]

X_test=df2_2.copy()

X_train=pd.get_dummies(X_train,drop_first=True)

X_test=pd.get_dummies(X_test,drop_first=True)

X_train.drop(["total_income","ApplicantIncome","CoapplicantIncome"],axis=1,inplace=True)

X_test.drop(["total_income","ApplicantIncome","CoapplicantIncome"],axis=1,inplace=True)

from sklearn.linear_model import LogisticRegression

X_train.columns.nunique()

LR=LogisticRegression()
LR.fit(X_train,Y_train)

X_test.rename(columns={'TI_log': 'ti_log'}, inplace=True)

X_test['Dependents_0'] = 0

X_test = X_test[X_train.columns]

y_pred = LR.predict(X_test)

prediction=pd.concat([df2.Loan_ID,pd.DataFrame(y_pred)],axis=1)

prediction.columns=['Loan_ID','Loan_Status']

prediction.to_csv('submission.csv',index=False)

import os
os.getcwd()

# prompt: give code for random forest using the above data

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


# Split the training data into training and validation sets
X_train_split, X_val_split, Y_train_split, Y_val_split = train_test_split(
    X_train, Y_train, test_size=0.2, random_state=42
)

# Create a Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train_split, Y_train_split)

# Make predictions on the validation set
Y_val_pred = rf_model.predict(X_val_split)

# Evaluate the model's performance
accuracy = accuracy_score(Y_val_split, Y_val_pred)
print("Validation Accuracy:", accuracy)

# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test)

# Create a submission file
prediction_rf = pd.concat([df2.Loan_ID, pd.DataFrame(y_pred_rf)], axis=1)
prediction_rf.columns = ['Loan_ID', 'Loan_Status']
prediction_rf.to_csv('submission_rf.csv', index=False)

# Make predictions on the validation set
Y_val_pred_xgb = xgb_model.predict(X_val_split)

# Evaluate the model's performance
# NOTE: removed line converting predictions to be the same type as Y_val_split
accuracy_xgb = accuracy_score(Y_val_split, Y_val_pred_xgb)
print("Validation Accuracy (XGBoost):", accuracy_xgb)


# Make predictions on the test set
y_pred_xgb = xgb_model.predict(X_test)

# Convert the predictions to 'Y' and 'N'
# NOTE: moved this conversion to AFTER the accuracy is calculated
y_pred_xgb = le.inverse_transform(y_pred_xgb)


# Create a submission file
prediction_xgb = pd.concat([df2.Loan_ID, pd.DataFrame(y_pred_xgb)], axis=1)
prediction_xgb.columns = ['Loan_ID', 'Loan_Status']
prediction_xgb.to_csv('submission_xgb1.csv', index=False)

# prompt: descision tree code

from sklearn.tree import DecisionTreeClassifier

# Create a Decision Tree classifier
dt_model = DecisionTreeClassifier(random_state=42)

# Train the model
dt_model.fit(X_train_split, Y_train_split)

# Make predictions on the validation set
Y_val_pred_dt = dt_model.predict(X_val_split)

# Evaluate the model's performance
accuracy_dt = accuracy_score(Y_val_split, Y_val_pred_dt)
print("Validation Accuracy (Decision Tree):", accuracy_dt)

# Make predictions on the test set
y_pred_dt = dt_model.predict(X_test)

# Create a submission file
prediction_dt = pd.concat([df2.Loan_ID, pd.DataFrame(y_pred_dt)], axis=1)
prediction_dt.columns = ['Loan_ID', 'Loan_Status']
prediction_dt.to_csv('submission_dt.csv', index=False)

# prompt: Labels in y_true and y_pred should be of the same type. Got y_true=['N' 'Y'] and y_pred=[0 1]. Make sure that the predictions provided by the classifier coincides with the true labels for the descion trre code baove

from sklearn.preprocessing import LabelEncoder

# ... (Your existing code) ...

# Create a Decision Tree classifier
dt_model = DecisionTreeClassifier(random_state=42)

# Train the model
dt_model.fit(X_train_split, Y_train_split)

# Make predictions on the validation set
Y_val_pred_dt = dt_model.predict(X_val_split)

# Convert the predictions to the same type as Y_val_split (using LabelEncoder)
le = LabelEncoder()
Y_train_encoded = le.fit_transform(Y_train_split)
Y_val_pred_dt_encoded = le.transform(Y_val_pred_dt)

# Evaluate the model's performance
accuracy_dt = accuracy_score(Y_val_split, Y_val_pred_dt) # Changed Y_train_encoded to Y_val_split
print("Validation Accuracy (Decision Tree):", accuracy_dt)

# Make predictions on the test set
y_pred_dt = dt_model.predict(X_test)

# Convert the predictions to 'Y' and 'N'
y_pred_dt = le.inverse_transform(y_pred_dt)

# Create a submission file
prediction_dt = pd.concat([df2.Loan_ID, pd.DataFrame(y_pred_dt)], axis=1)
prediction_dt.columns = ['Loan_ID', 'Loan_Status']
prediction_dt.to_csv('submission_dt1.csv', index=False)